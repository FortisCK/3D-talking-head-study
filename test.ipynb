{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\envs\\ff\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import copy\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "\n",
    "from torch.utils import data\n",
    "from pathlib import Path\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms as T, utils\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "from einops_exts import check_shape, rearrange_many\n",
    "\n",
    "from rotary_embedding_torch import RotaryEmbedding\n",
    "\n",
    "from video_diffusion_pytorch.text import tokenize, bert_embed, BERT_MODEL_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = torch.randn(2, 3, 5, 32, 32) # video (batch, channels, frames, height, width)\n",
    "text = torch.randn(2, 64)             # assume output of BERT-large has dimension of 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from video_diffusion_pytorch import Unet3D, GaussianDiffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss = diffusion(videos, cond = text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(t):\n",
    "    return t * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = videos\n",
    "image_size = 32\n",
    "channels = 3\n",
    "num_frames = 5\n",
    "num_timesteps = 1000\n",
    "timesteps = 1000\n",
    "loss_type = 'l1'\n",
    "\n",
    "b, device, img_size, = x.shape[0], x.device, image_size\n",
    "check_shape(x, 'b c f h w', c = channels, f = num_frames, h = img_size, w = img_size)\n",
    "# 随机生成一行b列的数据，从0到1000\n",
    "t = torch.randint(0, num_timesteps, (b,), device=device).long()\n",
    "x = normalize_img(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p_losses(x, t, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(x):\n",
    "    return x is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if callable(d) else d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_start = x\n",
    "b, c, f, h, w, device = *x_start.shape, x_start.device\n",
    "noise = default(None, lambda: torch.randn_like(x_start))\n",
    "#x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 余弦schedule\n",
    "def cosine_beta_schedule(timesteps, s = 0.008):\n",
    "    \"\"\"\n",
    "    cosine schedule\n",
    "    as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps, dtype = torch.float64)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0, 0.9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = cosine_beta_schedule(timesteps)\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.)\n",
    "\n",
    "timesteps, = betas.shape\n",
    "num_timesteps = int(timesteps)\n",
    "loss_type = loss_type\n",
    "\n",
    "\n",
    "# register_buffer = lambda name, val: register_buffer(name, val.to(torch.float32))\n",
    "\n",
    "# register_buffer('betas', betas)\n",
    "# register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "# register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n",
    "\n",
    "# # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "\n",
    "# register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "# register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod))\n",
    "# register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod))\n",
    "# register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod))\n",
    "# register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1))\n",
    "\n",
    "# # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "\n",
    "# posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "\n",
    "# # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n",
    "\n",
    "# register_buffer('posterior_variance', posterior_variance)\n",
    "\n",
    "# # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n",
    "\n",
    "# register_buffer('posterior_log_variance_clipped', torch.log(posterior_variance.clamp(min =1e-20)))\n",
    "# register_buffer('posterior_mean_coef1', betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))\n",
    "# register_buffer('posterior_mean_coef2', (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_buffer = lambda name, val: register_buffer(name, val.to(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "register_buffer('betas', betas)\n",
    "# register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "# register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_sample(x_start, t, noise = None):\n",
    "    noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "\n",
    "    return (\n",
    "        extract(sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n",
    "        extract(sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_list_str(x):\n",
    "    if not isinstance(x, (list, tuple)):\n",
    "        return False\n",
    "    return all([type(el) == str for el in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_losses(x_start, t, cond = None, noise = None, **kwargs):\n",
    "        b, c, f, h, w, device = *x_start.shape, x_start.device\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "\n",
    "        x_noisy = q_sample(x_start=x_start, t=t, noise=noise)\n",
    "\n",
    "        if is_list_str(cond):\n",
    "            cond = bert_embed(tokenize(cond), return_cls_repr = self.text_use_bert_cls)\n",
    "            cond = cond.to(device)\n",
    "\n",
    "        x_recon = denoise_fn(x_noisy, t, cond = cond, **kwargs)\n",
    "\n",
    "        if loss_type == 'l1':\n",
    "            loss = F.l1_loss(noise, x_recon)\n",
    "        elif loss_type == 'l2':\n",
    "            loss = F.mse_loss(noise, x_recon)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet3D(\n",
    "    dim = 64,\n",
    "    cond_dim = 64,\n",
    "    dim_mults = (1, 2, 4, 8)\n",
    ")\n",
    "diffusion = GaussianDiffusion(\n",
    "    model,\n",
    "    image_size = 32,\n",
    "    num_frames = 5,\n",
    "    timesteps = 1000,   # number of steps\n",
    "    loss_type = 'l1'    # L1 or L2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = diffusion(videos, cond = text)\n",
    "x = normalize_img(videos)\n",
    "t = torch.randint(0, num_timesteps, (b,), device=device).long()\n",
    "noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "x = diffusion.q_sample(videos, t, noise=noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_mask_like(shape, prob, device):\n",
    "    if prob == 1:\n",
    "        return torch.ones(shape, device = device, dtype = torch.bool)\n",
    "    elif prob == 0:\n",
    "        return torch.zeros(shape, device = device, dtype = torch.bool)\n",
    "    else:\n",
    "        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_present_mask = None\n",
    "prob_focus_present = 0\n",
    "batch, device = x.shape[0], x.device\n",
    "focus_present_mask = default(focus_present_mask, lambda: prob_mask_like((batch,), prob_focus_present, device = device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativePositionBias(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        heads = 8,\n",
    "        num_buckets = 32,\n",
    "        max_distance = 128\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_buckets = num_buckets\n",
    "        self.max_distance = max_distance\n",
    "        self.relative_attention_bias = nn.Embedding(num_buckets, heads)\n",
    "\n",
    "    @staticmethod\n",
    "    def _relative_position_bucket(relative_position, num_buckets = 32, max_distance = 128):\n",
    "        ret = 0\n",
    "        n = -relative_position\n",
    "\n",
    "        num_buckets //= 2\n",
    "        ret += (n < 0).long() * num_buckets\n",
    "        n = torch.abs(n)\n",
    "\n",
    "        max_exact = num_buckets // 2\n",
    "        is_small = n < max_exact\n",
    "\n",
    "        val_if_large = max_exact + (\n",
    "            torch.log(n.float() / max_exact) / math.log(max_distance / max_exact) * (num_buckets - max_exact)\n",
    "        ).long()\n",
    "        val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1))\n",
    "\n",
    "        ret += torch.where(is_small, n, val_if_large)\n",
    "        return ret\n",
    "\n",
    "    def forward(self, n, device):\n",
    "        q_pos = torch.arange(n, dtype = torch.long, device = device)\n",
    "        k_pos = torch.arange(n, dtype = torch.long, device = device)\n",
    "        rel_pos = rearrange(k_pos, 'j -> 1 j') - rearrange(q_pos, 'i -> i 1')\n",
    "        rp_bucket = self._relative_position_bucket(rel_pos, num_buckets = self.num_buckets, max_distance = self.max_distance)\n",
    "        values = self.relative_attention_bias(rp_bucket)\n",
    "        return rearrange(values, 'i j h -> h i j')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_rel_pos_bias = RelativePositionBias(heads = 8, max_distance = 32)\n",
    "time_rel_pos_bias = time_rel_pos_bias(x.shape[2], device = x.device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ff')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3583ec64756496a24aa5b79ca068089edc9847428e43a337e83fd0016c005afc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
